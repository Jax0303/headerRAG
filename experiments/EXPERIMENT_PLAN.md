# 실험 계획서

## 1. 데이터셋 추천

### 우선순위별 추천 데이터셋

#### 1순위: 공공데이터포털 (data.go.kr)
- **데이터 유형**: 기업 재무제표, 통계 데이터, 정책 데이터
- **장점**: 
  - 실제 한국 기업에서 사용하는 표 형식
  - 복잡한 구조 (병합 셀, 중첩 헤더 등)
  - 대규모 데이터 (수천~수만 개 테이블)
  - 다양한 도메인 (금융, 제조, 서비스 등)
- **추천 데이터셋**:
  - 상장기업 재무정보 (전자공시)
  - 국가통계 데이터
  - 산업통계 데이터

#### 2순위: 금융감독원 DART (dart.fss.or.kr)
- **데이터 유형**: 기업 공시 데이터, 재무제표
- **장점**:
  - 매우 복잡한 표 구조 (XBRL 형식)
  - 실제 기업 데이터
  - 거대한 규모 (수십만 개 테이블)
- **형식**: XBRL, Excel

#### 3순위: KOSIS 국가통계포털 (kosis.kr)
- **데이터 유형**: 국가 통계 데이터
- **장점**:
  - 복잡한 시계열 표
  - 다양한 산업 분야
  - 정기적으로 업데이트

## 2. 핵심 실험

### 실험 1: 레이블링 기반 파싱 vs Naive 파싱

**가설**: 각 데이터셀, 헤더셀, 열 셀에 레이블을 부착하면 파싱 성능이 향상된다.

**평가 지표**:
- 헤더 감지 정확도
- 데이터 셀 추출 정확도
- 구조 인식 정확도
- 시맨틱 레이블 품질

**실험 설계**:
1. 동일한 테이블에 대해 두 가지 파싱 방식 적용
2. 파싱 결과 비교
3. 구조 복잡도별 성능 분석

### 실험 2: KG 기반 RAG vs Naive 파싱 RAG

**가설**: 테이블을 Knowledge Graph로 변환한 후 RAG를 수행하면 검색 및 답변 생성 성능이 향상된다.

**평가 지표**:
- 검색 정확도 (Precision@K, Recall@K, MRR)
- 답변 정확도 (Exact Match, Partial Match)
- ROUGE 스코어 (생성된 텍스트 품질)
- 응답 시간

**실험 설계**:
1. 동일한 테이블 집합에 대해 두 가지 RAG 시스템 구축
2. 동일한 쿼리 세트로 검색 성능 비교
3. 다양한 질의 유형별 성능 분석

## 3. 추가 실험 아이디어

### 실험 3: 표 구조 복잡도에 따른 성능 비교
- **목적**: 표 구조의 복잡도가 파싱 및 RAG 성능에 미치는 영향 분석
- **변수**: 
  - 단순 표 (2차원, 헤더 1개)
  - 중첩 헤더 표 (다중 헤더 행/열)
  - 병합 셀 표 (rowspan, colspan)
  - 비정형 표 (불규칙한 구조)
- **평가**: 각 구조 유형별 파싱 정확도 및 RAG 성능

### 실험 4: 도메인별 성능 평가
- **목적**: 도메인 특성이 RAG 성능에 미치는 영향 분석
- **도메인**:
  - 금융 데이터 (재무제표, 주가 정보)
  - 의료 데이터 (진단 기록, 실험 데이터)
  - 교육 데이터 (성적표, 통계)
  - 제조 데이터 (생산 통계, 품질 데이터)
- **평가**: 도메인별 검색 정확도 및 답변 품질

### 실험 5: 노이즈 수준에 따른 성능 비교
- **목적**: 데이터 품질이 파싱 및 RAG 성능에 미치는 영향 분석
- **노이즈 유형**:
  - 완벽한 표 (오류 없음)
  - 오탈자 포함 표
  - 누락된 값이 있는 표
  - 불완전한 구조 (병합 셀 누락 등)
- **평가**: 노이즈 수준별 성능 저하 정도

### 실험 6: 임베딩 모델 비교
- **목적**: 한국어 특화 모델이 성능에 미치는 영향 분석
- **모델 비교**:
  - 다국어 모델 (paraphrase-multilingual-MiniLM-L12-v2)
  - 한국어 특화 모델 (KoBERT, KoGPT, KLUE-BERT)
  - 최신 LLM 임베딩 (OpenAI text-embedding-ada-002)
- **평가**: 임베딩 품질 및 검색 성능

### 실험 7: 질의 유형별 성능 분석
- **목적**: 질의의 복잡도가 RAG 성능에 미치는 영향 분석
- **질의 유형**:
  - 단순 조회 질의 ("2023년 매출액은?")
  - 집계 질의 ("부서별 평균 연봉은?")
  - 비교 질의 ("작년 대비 성장률이 높은 항목은?")
  - 추론 질의 ("업계 평균 대비 우리 회사 위치는?")
- **평가**: 질의 유형별 정확도 및 응답 시간

### 실험 8: 파싱 전처리 기법 비교
- **목적**: 전처리 방법이 파싱 정확도에 미치는 영향 분석
- **전처리 기법**:
  - OCR 후처리 (이미지에서 추출한 표)
  - 직접 파싱 (Excel/CSV에서 직접 로드)
  - 표 구조 인식 (컴퓨터 비전 기반)
- **평가**: 전처리 방법별 파싱 정확도 및 RAG 성능

### 실험 9: 컨텍스트 윈도우 크기 영향 분석
- **목적**: RAG에서 사용하는 컨텍스트 크기가 성능에 미치는 영향 분석
- **변수**: Top-K 검색 결과 수 (K=1, 3, 5, 10)
- **평가**: K 값별 검색 정확도 및 답변 품질

### 실험 10: 하이브리드 접근법 평가
- **목적**: KG 기반과 Naive 파싱을 결합한 하이브리드 방식 평가
- **방법**:
  - 앙상블 검색 (두 방식의 결과 결합)
  - 가중 평균 스코어링
  - 두 단계 검색 (KG로 후보 선별 → Naive로 정밀 검색)
- **평가**: 하이브리드 방식의 성능 향상 정도

## 4. 평가 체계

### 파싱 평가
- **정확도 메트릭**: 헤더 감지 정확도, 셀 추출 정확도
- **구조 메트릭**: 구조 복원 정확도, 관계 인식 정확도
- **품질 메트릭**: 시맨틱 레이블 품질, 노이즈 내성

### RAG 평가
- **검색 메트릭**: Precision@K, Recall@K, MRR, NDCG
- **답변 메트릭**: Exact Match, Partial Match, F1 Score
- **생성 메트릭**: ROUGE-1/2/L, BLEU, BERTScore
- **효율성 메트릭**: 응답 시간, 인덱스 구축 시간

## 5. 데이터 수집 가이드

### 공공데이터포털 사용법
1. https://www.data.go.kr 접속
2. "데이터셋" 메뉴에서 검색
3. Excel/CSV 형식 데이터 다운로드
4. 복잡한 표 구조가 있는 데이터 우선 선택

### DART 사용법
1. https://dart.fss.or.kr 접속
2. "공시 검색" 메뉴 활용
3. 재무제표 데이터 다운로드
4. XBRL 파서 필요 (추가 구현 가능)

## 6. 실행 계획

1. **1주차**: 데이터 수집 및 전처리
2. **2주차**: 파싱 모듈 구현 및 실험 1 실행
3. **3주차**: KG 변환 및 RAG 시스템 구현
4. **4주차**: 실험 2 실행 및 추가 실험 설계
5. **5주차**: 추가 실험 실행 및 결과 분석
6. **6주차**: 보고서 작성 및 개선 사항 도출

