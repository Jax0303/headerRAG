# ë¼ë²¨ë§ ì‹œìŠ¤í…œ ê°œì„  ë¡œë“œë§µ

## í˜„ì¬ ì‹œìŠ¤í…œ í•œê³„ì 

### 1. í—¤ë” ê°ì§€
- **í˜„ì¬**: ì²« í–‰/ì—´ì˜ í…ìŠ¤íŠ¸ ë¹„ìœ¨ë§Œ í™•ì¸ (50% ê¸°ì¤€)
- **ë¬¸ì œ**: 
  - ë‹¤ì¤‘ í—¤ë” í–‰/ì—´ ê°ì§€ ë¶ˆê°€ (ì˜ˆ: 2-3ì¤„ í—¤ë”)
  - ì¤‘ì²© í—¤ë” ì²˜ë¦¬ ë¶ˆê°€ (ì˜ˆ: "2023ë…„ | ë§¤ì¶œ | êµ­ë‚´/í•´ì™¸")
  - ë¹„ì •í˜• í—¤ë” êµ¬ì¡° ì²˜ë¦¬ ë¶ˆê°€

### 2. ì‹œë§¨í‹± ë ˆì´ë¸”
- **í˜„ì¬**: ë‹¨ìˆœ íŒ¨í„´ ë§¤ì¹­ (ì—°ë„, ê¸ˆì•¡, ë¹„ìœ¨)
- **ë¬¸ì œ**:
  - ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ì¸ì‹ ë¶ˆê°€ ("EBITDA", "ROE" ë“±)
  - ë¬¸ë§¥ ì´í•´ ì—†ìŒ ("Q1" vs "1ë¶„ê¸°")
  - ë‹¤êµ­ì–´ ì§€ì› ì œí•œì 

### 3. ë³‘í•© ì…€ ê°ì§€
- **í˜„ì¬**: ê°™ì€ ê°’ ì—°ì†ë§Œ í™•ì¸
- **ë¬¸ì œ**:
  - NaNìœ¼ë¡œ ë³‘í•©ëœ ì…€ë§Œ ê°ì§€ ê°€ëŠ¥
  - ì‹¤ì œ ë³‘í•© ì •ë³´ ì—†ìœ¼ë©´ ì‹¤íŒ¨
  - ë¶ˆê·œì¹™í•œ ë³‘í•© êµ¬ì¡° ì²˜ë¦¬ ë¶ˆê°€

### 4. ë³µì¡í•œ í‘œ êµ¬ì¡°
- **í˜„ì¬**: ë‹¨ìˆœ 2ì°¨ì› ë°°ì—´ ì²˜ë¦¬
- **ë¬¸ì œ**:
  - ìŠ¤íŒ¬ í—¤ë” ì²˜ë¦¬ ë¶ˆê°€
  - ì…€ ì •ë ¬ ì •ë³´ ë¯¸í™œìš© (ì‹œê°ì  íŒíŠ¸)
  - ê³„ì¸µì  êµ¬ì¡° ì²˜ë¦¬ ë¶ˆê°€

---

## ê°œì„  ìš°ì„ ìˆœìœ„ ë° ë°©ì•ˆ

### ğŸ¯ Phase 1: ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥ (ê·œì¹™ ê¸°ë°˜ ê°•í™”)

#### 1-1. ì •ê·œí‘œí˜„ì‹ í™•ì¥
```python
# í˜„ì¬: ì—°ë„, ê¸ˆì•¡, ë¹„ìœ¨ë§Œ
# ê°œì„ : ë” ë§ì€ íŒ¨í„´ ì¶”ê°€

íŒ¨í„´_ì‚¬ì „ = {
    'ë‚ ì§œ': r'\d{4}[.-]\d{1,2}[.-]\d{1,2}',
    'ì‹œê°„': r'\d{1,2}:\d{2}',
    'ë°±ë¶„ìœ¨': r'\d+\.?\d*%',
    'í†µí™”': r'[â‚©$â‚¬Â£Â¥]\s*\d+',
    'ë‹¨ìœ„': r'\d+\s*(kg|g|m|cm|km|l|ml)',
    # í•œêµ­ì–´ íŠ¹í™”
    'ì—°ë„_í•œêµ­ì–´': r'\d{4}ë…„',
    'ë¶„ê¸°': r'[1-4]ë¶„ê¸°|Q[1-4]',
    'ì›”': r'\d{1,2}ì›”',
    # ë„ë©”ì¸ë³„
    'ì¬ë¬´ì§€í‘œ': r'(ROE|ROA|EBITDA|PER|PBR)',
    'í†µê³„': r'(í‰ê· |ì¤‘ì•™ê°’|í‘œì¤€í¸ì°¨)',
}
```

**êµ¬í˜„ í¬ì¸íŠ¸**:
- ì •ê·œí‘œí˜„ì‹ ì‚¬ì „ êµ¬ì¶•
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë§¤ì¹­ (ë” êµ¬ì²´ì ì¸ ê²ƒ ë¨¼ì €)
- í•œêµ­ì–´ íŠ¹í™” íŒ¨í„´ ì¶”ê°€

#### 1-2. í†µê³„ ê¸°ë°˜ í—¤ë” ê°ì§€ ê°œì„ 
```python
# í˜„ì¬: í…ìŠ¤íŠ¸ ë¹„ìœ¨ë§Œ í™•ì¸
# ê°œì„ : ë‹¤ì¤‘ ì‹ í˜¸ í†µí•©

def improved_header_detection(table):
    ì‹ í˜¸ë“¤ = {
        'í…ìŠ¤íŠ¸_ë¹„ìœ¨': calculate_text_ratio(row),
        'ê°’_ì¼ê´€ì„±': calculate_value_consistency(row),
        'ë°ì´í„°_íƒ€ì…_ë¶„í¬': analyze_dtype_distribution(row),
        'ìœ„ì¹˜_ê°€ì¤‘ì¹˜': calculate_position_weight(row_index),
        'ì£¼ë³€_ì…€_íŒ¨í„´': analyze_context_pattern(row)
    }
    
    ì ìˆ˜ = ê°€ì¤‘_í•©ê³„(ì‹ í˜¸ë“¤)
    return ì ìˆ˜ > threshold
```

**êµ¬í˜„ í¬ì¸íŠ¸**:
- ì—¬ëŸ¬ íŒíŠ¸ í†µí•© (í…ìŠ¤íŠ¸ ë¹„ìœ¨ + ë°ì´í„° íƒ€ì… + ìœ„ì¹˜)
- í–‰/ì—´ë³„ í†µê³„ ë¶„ì„
- ì´ìƒê°’ íƒì§€ í™œìš©

#### 1-3. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë ˆì´ë¸”ë§
```python
# í˜„ì¬: ì…€ í•˜ë‚˜ë§Œ ë´„
# ê°œì„ : ì£¼ë³€ ì…€ ê³ ë ¤

def contextual_labeling(cell, row, col, table):
    # ê°™ì€ í–‰/ì—´ íŒ¨í„´ ë¶„ì„
    ê°™ì€_í–‰_íŒ¨í„´ = analyze_row_pattern(table[row, :])
    ê°™ì€_ì—´_íŒ¨í„´ = analyze_col_pattern(table[:, col])
    
    # ì£¼ë³€ ì…€ ê°’ ê³ ë ¤
    ì£¼ë³€_ì…€ = get_neighbors(row, col, table)
    
    # ì „ì²´ í…Œì´ë¸” êµ¬ì¡° ê³ ë ¤
    êµ¬ì¡°_íŒ¨í„´ = analyze_table_structure(table)
    
    return integrated_label(cell, ê°™ì€_í–‰_íŒ¨í„´, ê°™ì€_ì—´_íŒ¨í„´, ì£¼ë³€_ì…€, êµ¬ì¡°_íŒ¨í„´)
```

**êµ¬í˜„ í¬ì¸íŠ¸**:
- í–‰/ì—´ë³„ íŒ¨í„´ ë¶„ì„
- ì£¼ë³€ ì…€ ê°’ í™œìš©
- ì „ì²´ êµ¬ì¡° ë¨¼ì € íŒŒì•… í›„ ê°œë³„ ì…€ ë ˆì´ë¸”ë§

---

### ğŸš€ Phase 2: ë¨¸ì‹ ëŸ¬ë‹ í†µí•©

#### 2-1. Transformer ê¸°ë°˜ í—¤ë” ë¶„ë¥˜
```python
from transformers import AutoTokenizer, AutoModel
from sklearn.linear_model import LogisticRegression

class MLHeaderDetector:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')
        self.model = AutoModel.from_pretrained('klue/bert-base')
        self.classifier = LogisticRegression()
    
    def extract_features(self, cell, context):
        # ì…€ ê°’ ì„ë² ë”©
        cell_embedding = self.model.encode(cell.value)
        
        # ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”© (ì£¼ë³€ ì…€ë“¤)
        context_embedding = self.model.encode(context)
        
        # ìœ„ì¹˜ ì •ë³´
        position_features = [cell.row, cell.col]
        
        return np.concatenate([cell_embedding, context_embedding, position_features])
    
    def predict_header(self, cell, context):
        features = self.extract_features(cell, context)
        return self.classifier.predict(features)
```

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: `transformers`, `torch`, `sklearn`

**êµ¬í˜„ í¬ì¸íŠ¸**:
- í•œêµ­ì–´ BERT ëª¨ë¸ ì‚¬ìš© (klue/bert-base ë“±)
- ì…€ ì„ë² ë”© + ì»¨í…ìŠ¤íŠ¸ ì„ë² ë”© ê²°í•©
- ê°„ë‹¨í•œ ë¶„ë¥˜ê¸° (LogisticRegression)ë¡œ í—¤ë”/ë°ì´í„° ë¶„ë¥˜
- í•™ìŠµ ë°ì´í„°: ê¸°ì¡´ íŒŒì‹± ê²°ê³¼ í™œìš©

#### 2-2. LLM ê¸°ë°˜ ì‹œë§¨í‹± ë ˆì´ë¸”ë§
```python
from openai import OpenAI

class LLMSemanticLabeler:
    def __init__(self):
        self.client = OpenAI()
        
    def label_cell(self, cell, table_context):
        prompt = f"""
ë‹¤ìŒ í…Œì´ë¸” ì…€ì˜ ì˜ë¯¸ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”:
ì…€ ê°’: {cell.value}
ì»¨í…ìŠ¤íŠ¸: {table_context}

ê°€ëŠ¥í•œ ë ˆì´ë¸”:
- ì—°ë„, ë‚ ì§œ, ì‹œê°„
- ê¸ˆì•¡, í†µí™”, ë¹„ìœ¨
- ì¬ë¬´ì§€í‘œ (ROE, ROA, EBITDA ë“±)
- í†µê³„ (í‰ê· , í•©ê³„, ê°œìˆ˜ ë“±)
- ë„ë©”ì¸ íŠ¹í™” ìš©ì–´

ë‹µë³€ í˜•ì‹: ë ˆì´ë¸” ì´ë¦„ë§Œ
"""
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: `openai`, `langchain`

**êµ¬í˜„ í¬ì¸íŠ¸**:
- Few-shot learningìœ¼ë¡œ ì˜ˆì‹œ ì œê³µ
- ë„ë©”ì¸ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
- ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆê° (ê°™ì€ íŒ¨í„´ ì¬ì‚¬ìš©)
- Fallback: LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©

#### 2-3. ê·¸ë˜í”„ ì‹ ê²½ë§(GNN) ê¸°ë°˜ êµ¬ì¡° ë¶„ì„
```python
import torch
from torch_geometric.nn import GCNConv

class TableStructureGNN:
    def __init__(self):
        # ì…€ì„ ë…¸ë“œë¡œ, ì¸ì ‘ ê´€ê³„ë¥¼ ì—£ì§€ë¡œ
        self.gnn = GCNConv(...)
    
    def build_graph(self, table):
        # ê° ì…€ì„ ë…¸ë“œë¡œ
        nodes = []
        edges = []
        
        for i in range(rows):
            for j in range(cols):
                node_id = i * cols + j
                nodes.append(cell_features(i, j))
                
                # ì¸ì ‘ ì…€ê³¼ ì—°ê²°
                if j > 0:
                    edges.append((node_id, node_id - 1))  # ì™¼ìª½
                if i > 0:
                    edges.append((node_id, node_id - cols))  # ìœ„ìª½
        
        return nodes, edges
    
    def detect_structure(self, table):
        nodes, edges = self.build_graph(table)
        structure = self.gnn(nodes, edges)
        return structure
```

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: `torch_geometric`, `torch`

**êµ¬í˜„ í¬ì¸íŠ¸**:
- ì…€ ê°„ ê´€ê³„ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§
- GNNìœ¼ë¡œ êµ¬ì¡° íŒ¨í„´ í•™ìŠµ
- ì»¤ë®¤ë‹ˆí‹° íƒì§€ë¡œ í—¤ë” ê·¸ë£¹ ì°¾ê¸°

---

### ğŸ¨ Phase 3: ì»´í“¨í„° ë¹„ì „ í†µí•©

#### 3-1. Table Transformer ëª¨ë¸ í™œìš©
```python
from transformers import TableTransformerModel
import torchvision

class CVTableParser:
    def __init__(self):
        self.model = TableTransformerModel.from_pretrained('microsoft/table-transformer-structure-recognition')
    
    def parse_structure(self, table_image):
        # ì´ë¯¸ì§€ì—ì„œ í‘œ êµ¬ì¡° ì¶”ì¶œ
        result = self.model(table_image)
        
        # ì…€ ìœ„ì¹˜, í¬ê¸°, ë³‘í•© ì •ë³´ ì¶”ì¶œ
        cells = result['cells']
        
        for cell in cells:
            cell['bbox'] = cell.bounding_box
            cell['merged'] = detect_merge_from_bbox(cell, cells)
        
        return cells
```

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**: `detectron2`, `torchvision`, `transformers`

**êµ¬í˜„ í¬ì¸íŠ¸**:
- Microsoft Table Transformer ì‚¬ìš©
- ì…€ ê²½ê³„ ìƒì(bbox)ë¡œ ë³‘í•© ì…€ ê°ì§€
- ì •ë ¬ ì •ë³´ë¡œ êµ¬ì¡° ì´í•´

---

### ğŸ”„ Phase 4: í†µí•© ì ‘ê·¼ë²•

#### 4-1. ì•™ìƒë¸” ë©”ì„œë“œ
```python
class EnsembleLabeler:
    def __init__(self):
        self.rule_based = LabeledTableParser()
        self.ml_detector = MLHeaderDetector()
        self.llm_labeler = LLMSemanticLabeler()
        self.cv_parser = CVTableParser()
    
    def label_table(self, table):
        results = {}
        
        # 1. ê·œì¹™ ê¸°ë°˜
        results['rule'] = self.rule_based.parse(table)
        
        # 2. ML ê¸°ë°˜
        if has_gpu():
            results['ml'] = self.ml_detector.predict(table)
        
        # 3. LLM ê¸°ë°˜ (ì¼ë¶€ ì…€ë§Œ)
        results['llm'] = self.llm_labeler.label_uncertain_cells(table, results['rule'])
        
        # 4. CV ê¸°ë°˜ (ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²½ìš°)
        if has_image(table):
            results['cv'] = self.cv_parser.parse(table_image)
        
        # ê²°ê³¼ í†µí•© (ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· )
        return self.aggregate_results(results)
    
    def aggregate_results(self, results):
        # ê° ë°©ë²•ì˜ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        confidences = calculate_confidence(results)
        
        # ê°€ì¤‘ í‰ê· 
        final_labels = {}
        for method, labels in results.items():
            weight = confidences[method]
            for cell_id, label in labels.items():
                final_labels[cell_id] = weighted_average(
                    final_labels.get(cell_id, {}), 
                    label, 
                    weight
                )
        
        return final_labels
```

**êµ¬í˜„ í¬ì¸íŠ¸**:
- ë‹¤ì–‘í•œ ë°©ë²• ì¡°í•©
- ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì—¬ëŸ¬ ë°©ë²• ì¬í™•ì¸

---

## ë‹¨ê³„ë³„ êµ¬í˜„ ìˆœì„œ

### ë‹¨ê¸° (1-2ì£¼)
1. âœ… ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ í™•ì¥
2. âœ… í†µê³„ ê¸°ë°˜ í—¤ë” ê°ì§€ ê°œì„ 
3. âœ… ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë ˆì´ë¸”ë§ ì¶”ê°€

### ì¤‘ê¸° (1-2ê°œì›”)
1. Transformer ê¸°ë°˜ í—¤ë” ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
2. LLM ê¸°ë°˜ ì‹œë§¨í‹± ë ˆì´ë¸”ë§ í†µí•©
3. í‰ê°€ ë°ì´í„°ì…‹ êµ¬ì¶• ë° ì„±ëŠ¥ ì¸¡ì •

### ì¥ê¸° (3-6ê°œì›”)
1. GNN ê¸°ë°˜ êµ¬ì¡° ë¶„ì„ ëª¨ë¸ ê°œë°œ
2. CV ëª¨ë¸ í†µí•© (ì´ë¯¸ì§€ ê¸°ë°˜ í‘œ ì²˜ë¦¬)
3. ì•™ìƒë¸” ì‹œìŠ¤í…œ êµ¬ì¶• ë° ìµœì í™”

---

## êµ¬ì²´ì  ê°œì„  í¬ì¸íŠ¸ ìš”ì•½

### ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥í•œ ë¶€ë¶„
1. **íŒ¨í„´ ë§¤ì¹­ í™•ì¥**
   - ë‚ ì§œ, ì‹œê°„, ë‹¨ìœ„, ì¬ë¬´ì§€í‘œ ë“± íŒ¨í„´ ì¶”ê°€
   - í•œêµ­ì–´ íŠ¹í™” íŒ¨í„´ (ë…„, ì›”, ì¼, ë¶„ê¸° ë“±)

2. **í—¤ë” ê°ì§€ ê°œì„ **
   - í…ìŠ¤íŠ¸ ë¹„ìœ¨ + ë°ì´í„° íƒ€ì… + ìœ„ì¹˜ í†µí•©
   - ë‹¤ì¤‘ í—¤ë” í–‰ ê°ì§€ (2-3ì¤„ê¹Œì§€)

3. **ì»¨í…ìŠ¤íŠ¸ í™œìš©**
   - í–‰/ì—´ íŒ¨í„´ ë¶„ì„
   - ì£¼ë³€ ì…€ ê°’ ê³ ë ¤

### ML/ë”¥ëŸ¬ë‹ ì¶”ê°€ ì‹œ
1. **í•œêµ­ì–´ BERT í™œìš©**
   - klue/bert-baseë¡œ ì…€ ì„ë² ë”©
   - í—¤ë”/ë°ì´í„° ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ

2. **LLM í™œìš©**
   - GPT-4/Claudeë¡œ ì‹œë§¨í‹± ë ˆì´ë¸”ë§
   - Few-shot learning + ë„ë©”ì¸ í”„ë¡¬í”„íŠ¸

3. **CV ëª¨ë¸ í™œìš©**
   - Table Transformerë¡œ êµ¬ì¡° ì¶”ì¶œ
   - ì‹œê°ì  ì •ë³´ í™œìš©

### í†µí•© ì‹œ
1. **ì•™ìƒë¸” ì ‘ê·¼**
   - ê·œì¹™ + ML + LLM + CV í†µí•©
   - ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 

2. **ì ì§„ì  ê°œì„ **
   - ê°„ë‹¨í•œ í‘œëŠ” ê·œì¹™ ê¸°ë°˜ ë¹ ë¥´ê²Œ ì²˜ë¦¬
   - ë³µì¡í•œ í‘œë§Œ ML/LLM í™œìš©

---

## ì°¸ê³  ë…¼ë¬¸/ëª¨ë¸

- **Table Transformer**: Microsoft, êµ¬ì¡° ì¸ì‹ ì „ìš© ëª¨ë¸
- **TATR**: Table Transformer ê¸°ë°˜ í‘œ ì¸ì‹
- **RGPT**: í…Œì´ë¸” êµ¬ì¡° ì´í•´ìš© GPT ëª¨ë¸
- **TAPAS**: Google, í…Œì´ë¸” QA ëª¨ë¸

---

## ì˜ˆìƒ íš¨ê³¼

### ì •í™•ë„
- í˜„ì¬: ê°„ë‹¨í•œ í‘œ 70-80%, ë³µì¡í•œ í‘œ 40-50%
- ê°œì„  í›„: ê°„ë‹¨í•œ í‘œ 95%+, ë³µì¡í•œ í‘œ 75-85%

### ì²˜ë¦¬ ì†ë„
- ê·œì¹™ ê¸°ë°˜: 1000 tables/sec
- ML ì¶”ê°€: 100 tables/sec (GPU)
- LLM ì¶”ê°€: 10 tables/sec (API ì§€ì—°)
- í†µí•©: ë‹¨ìˆœ í‘œëŠ” ë¹ ë¥´ê²Œ, ë³µì¡í•œ í‘œë§Œ ëŠë¦¬ê²Œ ì²˜ë¦¬

### ë¹„ìš©
- ê·œì¹™ ê¸°ë°˜: ë¬´ë£Œ
- ML: GPU í•„ìš” (ë¡œì»¬/í´ë¼ìš°ë“œ)
- LLM: API ë¹„ìš© (ì…€ë‹¹ $0.001 ì •ë„)

