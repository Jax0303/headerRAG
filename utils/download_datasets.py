"""
ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹°
ë‹¤ì–‘í•œ í‘œ ë°ì´í„°ì…‹ ì§€ì›: PubTables-1M, TabRecSet, KorWikiTabular ë“±
"""

import os
import requests
import pandas as pd
from typing import List, Optional, Dict, Tuple
from pathlib import Path
import zipfile
import json
import subprocess
from tqdm import tqdm
import shutil


class DatasetDownloader:
    """ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë”"""
    
    def __init__(self, output_dir: str = "data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def download_public_data(self, 
                            dataset_url: str,
                            save_path: Optional[str] = None) -> str:
        """
        ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        
        Args:
            dataset_url: ë°ì´í„°ì…‹ URL
            save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if save_path is None:
            filename = dataset_url.split('/')[-1]
            save_path = self.output_dir / filename
        
        print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {dataset_url}")
        response = requests.get(dataset_url, stream=True)
        response.raise_for_status()
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"ì €ì¥ ì™„ë£Œ: {save_path}")
        return str(save_path)
    
    def download_pubtables1m(self, 
                            output_subdir: str = "pubtables1m",
                            use_sample: bool = True) -> Path:
        """
        PubTables-1M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (Microsoft Research)
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            use_sample: ìƒ˜í”Œ ë°ì´í„°ë§Œ ì‚¬ìš© ì—¬ë¶€ (ì „ì²´ëŠ” ë§¤ìš° í¼)
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("PubTables-1M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("="*70)
        print("\nğŸ“ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("1. GitHub ì €ì¥ì†Œ: https://github.com/microsoft/table-transformer")
        print("2. DatasetNinja: https://datasetninja.com/pubtables-1m")
        print("3. Microsoft Research: https://www.microsoft.com/en-us/research/publication/pubtables-1m/")
        print("\nğŸ’¡ íŠ¹ì§•:")
        print("  - ì•½ 100ë§Œ ê°œì˜ ê³¼í•™ ë…¼ë¬¸ í‘œ")
        print("  - ë³µì¡í•œ í‘œ êµ¬ì¡° ì •ë³´ í’ë¶€")
        print("  - í—¤ë” ë° ìœ„ì¹˜ ì •ë³´ í¬í•¨")
        print("\nâš ï¸  ì£¼ì˜: ì „ì²´ ë°ì´í„°ì…‹ì€ ë§¤ìš° í½ë‹ˆë‹¤ (ìˆ˜ì‹­ GB)")
        if use_sample:
            print("  â†’ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ê¶Œì¥")
        
        # ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ íŒŒì¼ ìƒì„±
        guide_path = dataset_dir / "DOWNLOAD_GUIDE.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write("""# PubTables-1M ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

## ë‹¤ìš´ë¡œë“œ ë°©ë²•

### ë°©ë²• 1: GitHub ì €ì¥ì†Œì—ì„œ
```bash
git clone https://github.com/microsoft/table-transformer.git
cd table-transformer
# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```

### ë°©ë²• 2: ì§ì ‘ ë‹¤ìš´ë¡œë“œ
- Microsoft Research í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ í™•ì¸
- DatasetNinjaì—ì„œ ë°ì´í„°ì…‹ ì •ë³´ í™•ì¸

## ë°ì´í„°ì…‹ íŠ¹ì§•
- ì•½ 100ë§Œ ê°œì˜ í‘œ
- ê³¼í•™ ë…¼ë¬¸ì—ì„œ ì¶”ì¶œ
- ë³µì¡í•œ í‘œ êµ¬ì¡° ì •ë³´ í’ë¶€
- í—¤ë” ë° ìœ„ì¹˜ ì •ë³´ í¬í•¨

## ì‚¬ìš© ë°©ë²•
```python
from utils.download_datasets import DatasetDownloader

downloader = DatasetDownloader()
pubtables_dir = downloader.download_pubtables1m(use_sample=True)
```
""")
        
        print(f"\nâœ… ê°€ì´ë“œ íŒŒì¼ ìƒì„±: {guide_path}")
        return dataset_dir
    
    def download_tabrecset(self,
                          output_subdir: str = "tabrecset") -> Path:
        """
        TabRecSet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (Figshare)
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("TabRecSet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("="*70)
        print("\nğŸ“ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("1. Figshare: https://figshare.com/articles/dataset/TabRecSet/...")
        print("\nğŸ’¡ íŠ¹ì§•:")
        print("  - ì´ì¤‘ì–¸ì–´ (ì˜ì–´/ì¤‘êµ­ì–´) í‘œ ë°ì´í„°")
        print("  - ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ í¬í•¨")
        print("  - ë‹¤ì–‘í•œ ë³µì¡ë„ ë ˆë²¨")
        
        # ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ íŒŒì¼ ìƒì„±
        guide_path = dataset_dir / "DOWNLOAD_GUIDE.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write("""# TabRecSet ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

## ë‹¤ìš´ë¡œë“œ ë°©ë²•
1. Figshareì—ì„œ ë°ì´í„°ì…‹ í˜ì´ì§€ ì ‘ì†
2. ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ í†µí•´ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
3. ì••ì¶• í•´ì œ í›„ ì´ ë””ë ‰í† ë¦¬ì— ì €ì¥

## ë°ì´í„°ì…‹ íŠ¹ì§•
- ì´ì¤‘ì–¸ì–´ (ì˜ì–´/ì¤‘êµ­ì–´) í‘œ ë°ì´í„°
- ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ í¬í•¨
- ë‹¤ì–‘í•œ ë³µì¡ë„ ë ˆë²¨

## ì‚¬ìš© ë°©ë²•
```python
from utils.download_datasets import DatasetDownloader

downloader = DatasetDownloader()
tabrecset_dir = downloader.download_tabrecset()
```
""")
        
        print(f"\nâœ… ê°€ì´ë“œ íŒŒì¼ ìƒì„±: {guide_path}")
        return dataset_dir
    
    def download_korwiki_tabular(self,
                                 output_subdir: str = "korwiki_tabular",
                                 github_repo: Optional[str] = None) -> Path:
        """
        KorWikiTabular/TQ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (GitHub)
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            github_repo: GitHub ì €ì¥ì†Œ URL (Noneì´ë©´ ê°€ì´ë“œë§Œ ìƒì„±)
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("KorWikiTabular/TQ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("="*70)
        print("\nğŸ“ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
        print("1. ë…¼ë¬¸ ì €ì¥ì†Œ GitHubì—ì„œ ë°ì´í„°ì…‹ í™•ì¸")
        print("2. í•´ë‹¹ ë…¼ë¬¸ì˜ ë°ì´í„°ì…‹ ë§í¬ ì°¸ì¡°")
        print("\nğŸ’¡ íŠ¹ì§•:")
        print("  - í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ í‘œ ë°ì´í„°")
        print("  - í•œêµ­ì–´ í‘œ êµ¬ì¡° íŠ¹í™”")
        print("  - TQ (Table Question) íƒœìŠ¤í¬ ì§€ì›")
        
        # ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ íŒŒì¼ ìƒì„±
        guide_path = dataset_dir / "DOWNLOAD_GUIDE.md"
        with open(guide_path, 'w', encoding='utf-8') as f:
            f.write("""# KorWikiTabular/TQ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

## ë‹¤ìš´ë¡œë“œ ë°©ë²•
1. ë…¼ë¬¸ì˜ GitHub ì €ì¥ì†Œì—ì„œ ë°ì´í„°ì…‹ ë§í¬ í™•ì¸
2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
3. ì••ì¶• í•´ì œ í›„ ì´ ë””ë ‰í† ë¦¬ì— ì €ì¥

## ë°ì´í„°ì…‹ íŠ¹ì§•
- í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ í‘œ ë°ì´í„°
- í•œêµ­ì–´ í‘œ êµ¬ì¡° íŠ¹í™”
- TQ (Table Question) íƒœìŠ¤í¬ ì§€ì›

## ì‚¬ìš© ë°©ë²•
```python
from utils.download_datasets import DatasetDownloader

downloader = DatasetDownloader()
korwiki_dir = downloader.download_korwiki_tabular()
```
""")
        
        if github_repo:
            try:
                print(f"\nGitHub ì €ì¥ì†Œ í´ë¡  ì‹œë„: {github_repo}")
                # Git í´ë¡ ì€ ì‚¬ìš©ìê°€ ì§ì ‘ í•´ì•¼ í•  ìˆ˜ë„ ìˆìŒ
                print("ğŸ’¡ í•„ìš”ì‹œ ì§ì ‘ git clone ëª…ë ¹ì–´ ì‹¤í–‰:")
                print(f"   git clone {github_repo} {dataset_dir}")
            except Exception as e:
                print(f"ê²½ê³ : ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print(f"\nâœ… ê°€ì´ë“œ íŒŒì¼ ìƒì„±: {guide_path}")
        return dataset_dir
    
    def download_rag_eval_ko(self, 
                             output_subdir: str = "rag_eval_ko",
                             use_huggingface: bool = True) -> Path:
        """
        RAG-Evaluation-Dataset-KO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (Hugging Face)
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            use_huggingface: Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("RAG-Evaluation-Dataset-KO ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("="*70)
        
        if use_huggingface:
            try:
                print("\nğŸ“¥ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                from datasets import load_dataset
                
                # ë°ì´í„°ì…‹ ë¡œë“œ
                dataset = load_dataset("allganize/RAG-Evaluation-Dataset-KO")
                
                # CSV íŒŒì¼ ì €ì¥
                if 'train' in dataset:
                    df_documents = dataset['train'].to_pandas()
                    df_documents.to_csv(dataset_dir / "documents.csv", index=False, encoding='utf-8')
                    print(f"âœ… documents.csv ì €ì¥ ì™„ë£Œ: {len(df_documents)}ê°œ ë¬¸ì„œ")
                
                # í‰ê°€ ê²°ê³¼ CSV (ìˆëŠ” ê²½ìš°)
                if 'test' in dataset:
                    df_eval = dataset['test'].to_pandas()
                    df_eval.to_csv(dataset_dir / "rag_evaluation_result.csv", index=False, encoding='utf-8')
                    print(f"âœ… rag_evaluation_result.csv ì €ì¥ ì™„ë£Œ: {len(df_eval)}ê°œ ì§ˆë¬¸")
                
                print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {dataset_dir}")
                return dataset_dir
                
            except ImportError:
                print("âš ï¸  datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   ì„¤ì¹˜: pip install datasets")
                use_huggingface = False
        
        if not use_huggingface:
            print("\nğŸ“ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë°©ë²•:")
            print("1. Gitìœ¼ë¡œ í´ë¡ :")
            print("   git clone https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO")
            print(f"2. ë˜ëŠ” ë‹¤ìš´ë¡œë“œ í›„ {dataset_dir}ì— ì €ì¥")
            
            # ê°€ì´ë“œ íŒŒì¼ ìƒì„±
            guide_path = dataset_dir / "DOWNLOAD_GUIDE.md"
            with open(guide_path, 'w', encoding='utf-8') as f:
                f.write("""# RAG-Evaluation-Dataset-KO ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œ

## ë‹¤ìš´ë¡œë“œ ë°©ë²•

### ë°©ë²• 1: Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
```python
from datasets import load_dataset
dataset = load_dataset("allganize/RAG-Evaluation-Dataset-KO")
```

### ë°©ë²• 2: Gitìœ¼ë¡œ í´ë¡ 
```bash
git clone https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO
```

### ë°©ë²• 3: ì§ì ‘ ë‹¤ìš´ë¡œë“œ
Hugging Face í˜ì´ì§€ì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ:
https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO

## ë°ì´í„°ì…‹ íŠ¹ì§•
- í•œêµ­ì–´ RAG í‰ê°€ ë°ì´í„°ì…‹
- 5ê°œ ë„ë©”ì¸ (finance, public, medical, law, commerce)
- 300ê°œ ì§ˆë¬¸
- PDF ë¬¸ì„œ í¬í•¨
""")
            print(f"\nâœ… ê°€ì´ë“œ íŒŒì¼ ìƒì„±: {guide_path}")
        
        return dataset_dir
    
    def download_pubtables1m_hf(self,
                                output_subdir: str = "pubtables1m",
                                num_samples: int = 1000) -> Path:
        """
        PubTables-1M ë°ì´í„°ì…‹ì„ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            num_samples: ë‹¤ìš´ë¡œë“œí•  ìƒ˜í”Œ ìˆ˜
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir / "data"
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print(f"PubTables-1M ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ìƒ˜í”Œ {num_samples}ê°œ)")
        print("="*70)
        
        try:
            print("\nğŸ“¥ Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            from datasets import load_dataset
            
            # ë°ì´í„°ì…‹ ë¡œë“œ (ìƒ˜í”Œë§Œ)
            split = f'train[:{num_samples}]'
            dataset = load_dataset('bsmock/pubtables-1m', split=split)
            
            print(f"ë‹¤ìš´ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: {len(dataset)}")
            
            # ë°ì´í„° ì €ì¥
            saved_count = 0
            for i, item in enumerate(tqdm(dataset, desc="ë°ì´í„° ì €ì¥")):
                json_path = dataset_dir / f'table_{i}.json'
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(item, f, ensure_ascii=False, indent=2)
                saved_count += 1
            
            print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {saved_count}ê°œ í…Œì´ë¸” ì €ì¥ë¨")
            print(f"   ì €ì¥ ìœ„ì¹˜: {dataset_dir}")
            return dataset_dir
            
        except ImportError:
            print("âš ï¸  datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì„¤ì¹˜: pip install datasets")
            return self.download_pubtables1m(output_subdir, use_sample=True)
        except Exception as e:
            print(f"âš ï¸  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
            return self.download_pubtables1m(output_subdir, use_sample=True)
    
    def download_tabrecset_from_url(self,
                                    output_subdir: str = "tabrecset",
                                    url: Optional[str] = None) -> Path:
        """
        TabRecSet ë°ì´í„°ì…‹ì„ URLì—ì„œ ë‹¤ìš´ë¡œë“œ
        
        Args:
            output_subdir: ì €ì¥í•  í•˜ìœ„ ë””ë ‰í† ë¦¬
            url: ë‹¤ìš´ë¡œë“œ URL (Noneì´ë©´ ê¸°ë³¸ URL ì‚¬ìš©)
        
        Returns:
            ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        dataset_dir = self.output_dir / output_subdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        print("="*70)
        print("TabRecSet ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ")
        print("="*70)
        
        # Figshare ë‹¤ìš´ë¡œë“œ URL (ì˜ˆì‹œ)
        if url is None:
            # ì‹¤ì œ URLì€ í™•ì¸ í•„ìš”
            url = "https://figshare.com/ndownloader/articles/20647788/versions/9"
        
        zip_path = dataset_dir / "tabrecset.zip"
        
        try:
            print(f"\nğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {url}")
            response = requests.get(url, stream=True, timeout=60)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            with open(zip_path, 'wb') as f, tqdm(
                desc="ë‹¤ìš´ë¡œë“œ",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        bar.update(len(chunk))
            
            print(f"\nâœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {zip_path}")
            
            # ì••ì¶• í•´ì œ
            print("ì••ì¶• í•´ì œ ì¤‘...")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(dataset_dir)
            
            print(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {dataset_dir}")
            
            # ZIP íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
            # zip_path.unlink()
            
            return dataset_dir
            
        except Exception as e:
            print(f"âš ï¸  ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
            return self.download_tabrecset(output_subdir)
    
    def save_metadata(self, tables_info: List[Dict], filename: str = "metadata.json"):
        """í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata_path = self.output_dir / filename
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(tables_info, f, ensure_ascii=False, indent=2)
        print(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")


def main():
    """ì˜ˆì œ ì‹¤í–‰"""
    downloader = DatasetDownloader()
    
    print("="*70)
    print("ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë”")
    print("="*70)
    
    print("\nğŸ“š ì§€ì›í•˜ëŠ” ë°ì´í„°ì…‹:")
    print("\n1. PubTables-1M (Microsoft Research)")
    print("   - ëŒ€ê·œëª¨ ê³¼í•™ ë…¼ë¬¸ í‘œ ë°ì´í„°ì…‹ (ì•½ 100ë§Œ ê°œ)")
    print("   - ë³µì¡í•œ í‘œ êµ¬ì¡° ì •ë³´ í’ë¶€")
    print("   - ì´ˆê¸° ì‹¤í—˜ì— ì¶”ì²œ")
    print("   ì‚¬ìš©ë²•: downloader.download_pubtables1m(use_sample=True)")
    
    print("\n2. TabRecSet (Figshare)")
    print("   - ì´ì¤‘ì–¸ì–´ (ì˜ì–´/ì¤‘êµ­ì–´) í‘œ ë°ì´í„°")
    print("   - ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ í¬í•¨")
    print("   - ì´ˆê¸° ì‹¤í—˜ì— ì¶”ì²œ")
    print("   ì‚¬ìš©ë²•: downloader.download_tabrecset()")
    
    print("\n3. KorWikiTabular/TQ (GitHub)")
    print("   - í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ í‘œ ë°ì´í„°")
    print("   - í•œêµ­ì–´ í‘œ êµ¬ì¡° íŠ¹í™”")
    print("   - í•œêµ­ ê¸°ì—… í‘œ íŠ¹í™” ì‹¤í—˜ì— ì¶”ì²œ")
    print("   ì‚¬ìš©ë²•: downloader.download_korwiki_tabular()")
    
    print("\n4. RAG-Evaluation-Dataset-KO (ê¸°ì¡´)")
    print("   - í•œêµ­ì–´ RAG í‰ê°€ ë°ì´í„°ì…‹")
    print("   - 5ê°œ ë„ë©”ì¸, 300ê°œ ì§ˆë¬¸")
    print("   ì‚¬ìš©ë²•: ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ì‚¬ìš©")
    
    print("\nğŸ“– ê³µê°œ ë°ì´í„°ì…‹:")
    print("1. ê³µê³µë°ì´í„°í¬í„¸: https://www.data.go.kr")
    print("2. DART: https://dart.fss.or.kr")
    print("3. KOSIS: https://kosis.kr")
    
    print("\nğŸ’¡ ì¶”ì²œ ì‚¬ìš© ì „ëµ:")
    print("  - ì´ˆê¸° ì‹¤í—˜: PubTables-1M (ìƒ˜í”Œ) ë˜ëŠ” TabRecSet")
    print("  - í•œêµ­ì–´ íŠ¹í™”: KorWikiTabular + RAG-Evaluation-Dataset-KO")
    print("  - ê·¹ë‹¨ ì¼€ì´ìŠ¤: TabRecSet")


if __name__ == "__main__":
    main()

